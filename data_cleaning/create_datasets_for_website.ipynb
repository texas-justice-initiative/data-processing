{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a minimalist, compressed version of CDR/OIS data for our website's explore-the-data app\n",
    "\n",
    "## NOTE: This is a temporary file, only existing until this code lives in a cron job somewhere\n",
    "\n",
    "### Purpose of this notebook\n",
    "\n",
    "This notebook generates the data files to that our explore the data page uses. There are two for each dataset:\n",
    "1. A compressed file with just enough data to show the charts, e.g. `cdr_compressed.json`\n",
    "2. The full CSV file, with all columns, in the same order as the compressed file, e.g. `cdr_full.csv`\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Simply run this notebook top to bottom to generate a bunch of new datafiles.\n",
    "\n",
    "### About the compressed file\n",
    "\n",
    "Say we have a set or records like this:\n",
    "```\n",
    "   [\n",
    "      {\"sex\": \"MALE\", \"race\": \"WHITE\", \"record_id\": \"PA111\"},\n",
    "      {\"sex\": \"MALE\", \"race\": \"HISPANIC\", \"record_id\": \"PA222\"},\n",
    "      {\"sex\": \"FEMALE\", \"race\": \"BLACK\", \"record_id\": \"PA333\"},\n",
    "      {\"sex\": \"FEMALE\", \"race\": null, \"record_id\": \"PA444\"},\n",
    "   ]\n",
    "```\n",
    "\n",
    "We will compress them to look like this:\n",
    "```\n",
    "    {\n",
    "      meta: {\n",
    "        lookups: {\n",
    "          \"sex\": [\"FEMALE\", \"MALE\"],\n",
    "          \"race\": [\"HISPANIC\", \"WHITE\", \"BLACK\"]\n",
    "        },\n",
    "        'record_ids': {\n",
    "            'field_name': 'record_id',\n",
    "            'values': [\"PA111\", \"PA222\", \"PA333\", \"PA444\"]\n",
    "        }\n",
    "        num_records: 3,\n",
    "        num_columns: 2\n",
    "      },\n",
    "      records: {\n",
    "        \"sex\": [1, 1, 0, 0],\n",
    "        \"race\": [1, 0, 2, -1]\n",
    "      },\n",
    "    }\n",
    "```\n",
    "Note that the 'records' object above contains indices in the lookup array for that column.  The value is -1 for missing values.\n",
    "\n",
    "In practice, this cuts our data size down dramatically by avoiding repeated keys or repeating long string values.\n",
    "\n",
    "##### Author: Everett Wetchler (everett.wetchler@gmail.com)\n",
    "\n",
    "## Edit this if you want to tweak what data ends up in the compressed file, or where it's written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MAPPING = {\n",
    "    'cdr': ['cdr'],\n",
    "    'ois': ['ois-civilians', 'ois-officers']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTFOLDER = './'  # Where to write the resulting files\n",
    "\n",
    "CONFIGS = {\n",
    "    'cdr': {\n",
    "        'DTW_PROJECT_KEY': 'tji/deaths-in-custody',\n",
    "        'DTW_FILENAME': 'cleaned_custodial_death_reports',\n",
    "        'OUTFILE_PREFIX': 'cdr',\n",
    "        'DATE_COL': 'death_date',\n",
    "        'ID_COL': 'record_id',\n",
    "        'KEEP_COLS': [\n",
    "            'record_id', 'year', 'race', 'sex', 'manner_of_death', 'age_at_time_of_death',\n",
    "            'type_of_custody', 'death_location_type', 'means_of_death', 'death_location_county', 'agency_name'\n",
    "        ]\n",
    "    },\n",
    "    'ois-civilians': {\n",
    "        'DTW_PROJECT_KEY': 'tji/officer-involved-shootings',\n",
    "        'DTW_FILENAME': 'shot_civilians',\n",
    "        'OUTFILE_PREFIX': 'ois',\n",
    "        'DATE_COL': 'date_incident',\n",
    "        'ID_COL': None,\n",
    "        'KEEP_COLS': [\n",
    "            'year', 'civilian_race', 'civilian_gender', 'civilian_age', 'civilian_died',\n",
    "            'officer_age_1', 'officer_race_1', 'officer_gender_1', 'incident_result_of',\n",
    "            'incident_county', 'agency_name_1', 'deadly_weapon',\n",
    "            'multiple_officers_involved'\n",
    "        ],\n",
    "        'RENAMES': {\n",
    "            'officer_gender_1': 'officer_gender',\n",
    "            'officer_age_1': 'officer_age',\n",
    "            'officer_race_1': 'officer_race',\n",
    "            'agency_name_1': 'agency_name',\n",
    "        }\n",
    "    },\n",
    "    'ois-officers': {\n",
    "        'DTW_PROJECT_KEY': 'tji/officer-involved-shootings',\n",
    "        'DTW_FILENAME': 'shot_officers',\n",
    "        'OUTFILE_PREFIX': 'ois_officers',\n",
    "        'DATE_COL': 'date_incident',\n",
    "        'ID_COL': None,\n",
    "        'KEEP_COLS': [\n",
    "            'year', 'civilian_race_1', 'civilian_gender_1', 'civilian_age_1', 'civilian_died',\n",
    "            'officer_age', 'officer_race', 'officer_gender', 'incident_county', 'agency_name_1'\n",
    "        ],\n",
    "        'RENAMES': {\n",
    "            'agency_name_1': 'agency_name',\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everett Wetchler 2018-12-02 01:06:11 CST\n",
      "\n",
      "numpy 1.15.3\n",
      "pandas 0.23.4\n",
      "datadotworld 1.6.0\n",
      "watermark 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import datadotworld as dw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import simplejson as json\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a \"Everett Wetchler\" -d -t -z -w -p numpy,pandas,datadotworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, id_col=None):\n",
    "    js = {\n",
    "        'meta': {\n",
    "            'num_columns': len(df.columns),\n",
    "            'num_records': len(df),\n",
    "            'lookups': {},\n",
    "        },\n",
    "        'records': {},\n",
    "    }\n",
    "    if id_col:\n",
    "        js['meta']['record_ids'] = {\n",
    "            'field_name': id_col,\n",
    "            'values': list(df[id_col])\n",
    "        }\n",
    "        df = df.drop(id_col, axis=1)\n",
    "    for col in df.columns:\n",
    "        values = sorted(list(set(df[col].dropna())))\n",
    "        mapping = dict((v, i) for i, v in enumerate(values))\n",
    "        js['meta']['lookups'][col] = values\n",
    "        js['records'][col] = df[col].apply(lambda x: -1 if pd.isnull(x) else mapping[x]).tolist()\n",
    "\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_s3(filename):\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3.Bucket('tji-compressed-data').upload_file(filename, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one(config, sample=False, s3_upload=False):\n",
    "    datasets = dw.load_dataset(config['DTW_PROJECT_KEY'], force_update=True)\n",
    "    df = datasets.dataframes[config['DTW_FILENAME']]\n",
    "    slim = df.copy()\n",
    "    slim['year'] = pd.to_datetime(slim[config['DATE_COL']]).dt.year\n",
    "    slim = slim[config['KEEP_COLS']]\n",
    "    slim.columns = [config.get('RENAMES', {}).get(c, c) for c in slim.columns]\n",
    "    prefix = \"\"\n",
    "    if sample:\n",
    "        slim = slim.sample(5)\n",
    "        prefix = \"SAMPLE_\"\n",
    "    compressed = compress(slim, id_col=config['ID_COL'])\n",
    "    \n",
    "    # Write\n",
    "    filename = OUTFOLDER + prefix + config['OUTFILE_PREFIX'] + '_compressed.json'\n",
    "    print(\"Writing full compressed file to\", filename)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(json.dumps(compress(slim, id_col=config['ID_COL'])))\n",
    "    if s3_upload:\n",
    "        write_to_s3(filename)\n",
    "    fullfile = OUTFOLDER + prefix + config['OUTFILE_PREFIX'] + '_full.csv'\n",
    "    print(\"Writing full file to \" + fullfile)\n",
    "    df.to_csv(fullfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For local development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdr\n",
      "Writing full compressed file to ./SAMPLE_cdr_compressed.json\n",
      "Writing full file to ./SAMPLE_cdr_full.csv\n",
      "Writing full compressed file to ./cdr_compressed.json\n",
      "Writing full file to ./cdr_full.csv\n",
      "ois-civilians\n",
      "Writing full compressed file to ./SAMPLE_ois_compressed.json\n",
      "Writing full file to ./SAMPLE_ois_full.csv\n",
      "Writing full compressed file to ./ois_compressed.json\n",
      "Writing full file to ./ois_full.csv\n",
      "ois-officers\n",
      "Writing full compressed file to ./SAMPLE_ois_officers_compressed.json\n",
      "Writing full file to ./SAMPLE_ois_officers_full.csv\n",
      "Writing full compressed file to ./ois_officers_compressed.json\n",
      "Writing full file to ./ois_officers_full.csv\n"
     ]
    }
   ],
   "source": [
    "for k, config in CONFIGS.items():\n",
    "    print(k)\n",
    "    create_one(config, sample=True)\n",
    "    create_one(config, sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For cronjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not writing to s3. To do so, set COMPRESS_CDR_S3 to 'TRUE'\n",
      "Not writing to s3. To do so, set COMPRESS_OIS_S3 to 'TRUE'\n"
     ]
    }
   ],
   "source": [
    "for dataset, configs in CONFIG_MAPPING.items():\n",
    "    if os.environ.get('COMPRESS_%s_S3' % dataset.upper()) != 'TRUE':\n",
    "        print(\"Not writing to s3. To do so, set COMPRESS_%s_S3 to 'TRUE'\" % dataset.upper())\n",
    "    else:\n",
    "        for config in configs:\n",
    "            create_one(CONFIGS[config], s3_upload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
