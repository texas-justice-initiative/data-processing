{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a minimalist, compressed version of CDR/OIS data for our website's explore-the-data app\n",
    "\n",
    "## NOTE: This is a temporary file, only existing until this code lives in a cron job somewhere\n",
    "\n",
    "### Purpose of this notebook\n",
    "\n",
    "This notebook generates the data files to that our explore the data page uses. There are two for each dataset:\n",
    "1. A compressed file with just enough data to show the charts, e.g. `cdr_compressed.json`\n",
    "2. The full CSV file, with all columns, in the same order as the compressed file, e.g. `cdr_full.csv`\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Simply run this notebook top to bottom to generate a bunch of new datafiles.\n",
    "\n",
    "### About the compressed file\n",
    "\n",
    "Say we have a set or records like this:\n",
    "```\n",
    "   [\n",
    "      {\"sex\": \"MALE\", \"race\": \"WHITE\", \"record_id\": \"PA111\"},\n",
    "      {\"sex\": \"MALE\", \"race\": \"HISPANIC\", \"record_id\": \"PA222\"},\n",
    "      {\"sex\": \"FEMALE\", \"race\": \"BLACK\", \"record_id\": \"PA333\"},\n",
    "      {\"sex\": \"FEMALE\", \"race\": null, \"record_id\": \"PA444\"},\n",
    "   ]\n",
    "```\n",
    "\n",
    "We will compress them to look like this:\n",
    "```\n",
    "    {\n",
    "      meta: {\n",
    "        lookups: {\n",
    "          \"sex\": [\"FEMALE\", \"MALE\"],\n",
    "          \"race\": [\"HISPANIC\", \"WHITE\", \"BLACK\"]\n",
    "        },\n",
    "        'record_ids': {\n",
    "            'field_name': 'record_id',\n",
    "            'values': [\"PA111\", \"PA222\", \"PA333\", \"PA444\"]\n",
    "        }\n",
    "        num_records: 3,\n",
    "        num_columns: 2\n",
    "      },\n",
    "      records: {\n",
    "        \"sex\": [1, 1, 0, 0],\n",
    "        \"race\": [1, 0, 2, -1]\n",
    "      },\n",
    "    }\n",
    "```\n",
    "Note that the 'records' object above contains indices in the lookup array for that column.  The value is -1 for missing values.\n",
    "\n",
    "In practice, this cuts our data size down dramatically by avoiding repeated keys or repeating long string values.\n",
    "\n",
    "In order to write compressed and slider files to s3, set the environment variables COMPRESS_CDR_S3 and/or COMPRESS_OIS_S3 to 'TRUE' before running this notebook.\n",
    "\n",
    "##### Author: Everett Wetchler (everett.wetchler@gmail.com),  Aiden Yang (alyang250@gmail.com), and Dashiel Lopez Mendez (hi@dashiel.dev)\n",
    "\n",
    "## Edit this if you want to tweak what data ends up in the compressed file, or where it's written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_MAPPING = {\n",
    "    'cdr': ['cdr'],\n",
    "    'ois': ['ois-civilians', 'ois-officers']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTFOLDER = './'  # Where to write the resulting files\n",
    "\n",
    "S3_BUCKET_NAME = 'tji-compressed-data'\n",
    "\n",
    "CONFIGS = {\n",
    "    'cdr': {\n",
    "        'DW_PROJECT_KEY': 'tji/deaths-in-custody',\n",
    "        'DW_FILENAME': 'cleaned_custodial_death_reports',\n",
    "        'OUTFILE_PREFIX': 'cdr',\n",
    "        'DATE_COL': 'death_date',\n",
    "        'ID_COL': 'record_id',\n",
    "        'KEEP_COLS': [\n",
    "            'record_id', 'year', 'race', 'sex', 'manner_of_death', 'age_at_time_of_death',\n",
    "            'type_of_custody', 'death_location_type', 'means_of_death', 'death_location_county', 'agency_name'\n",
    "        ]\n",
    "    },\n",
    "    'ois-civilians': {\n",
    "        'DW_PROJECT_KEY': 'tji/officer-involved-shootings',\n",
    "        'DW_FILENAME': 'shot_civilians',\n",
    "        'OUTFILE_PREFIX': 'ois',\n",
    "        'DATE_COL': 'date_incident',\n",
    "        'ID_COL': None,\n",
    "        'KEEP_COLS': [\n",
    "            'year', 'civilian_race', 'civilian_gender', 'civilian_age', 'civilian_died',\n",
    "            'officer_age_1', 'officer_race_1', 'officer_gender_1', 'incident_result_of',\n",
    "            'incident_county', 'agency_name_1', 'deadly_weapon',\n",
    "            'multiple_officers_involved'\n",
    "        ],\n",
    "        'RENAMES': {\n",
    "            'officer_gender_1': 'officer_gender',\n",
    "            'officer_age_1': 'officer_age',\n",
    "            'officer_race_1': 'officer_race',\n",
    "            'agency_name_1': 'agency_name',\n",
    "        }\n",
    "    },\n",
    "    'ois-officers': {\n",
    "        'DW_PROJECT_KEY': 'tji/officer-involved-shootings',\n",
    "        'DW_FILENAME': 'shot_officers',\n",
    "        'OUTFILE_PREFIX': 'ois_officers',\n",
    "        'DATE_COL': 'date_incident',\n",
    "        'ID_COL': None,\n",
    "        'KEEP_COLS': [\n",
    "            'year', 'civilian_race_1', 'civilian_gender_1', 'civilian_age_1', 'civilian_harm',\n",
    "            'officer_age', 'officer_race', 'officer_gender', 'officer_harm',\n",
    "            'incident_county', 'agency_name_1'\n",
    "        ],\n",
    "        'RENAMES': {\n",
    "            'agency_name_1': 'agency_name',\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "import datadotworld as dw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# %load_ext watermark\n",
    "# %watermark -a \"Everett Wetchler Aiden Yang, and Dashiel Lopez Mendez\" -d -t -z -w -p numpy,pandas,datadotworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_original(df, id_col=None):\n",
    "    js = {\n",
    "        'meta': {\n",
    "            'num_columns': len(df.columns),\n",
    "            'num_records': len(df),\n",
    "            'lookups': {},\n",
    "        },\n",
    "        'records': {},\n",
    "    }\n",
    "    if id_col:\n",
    "        js['meta']['record_ids'] = {\n",
    "            'field_name': id_col,\n",
    "            'values': list(df[id_col])\n",
    "        }\n",
    "        df = df.drop(id_col, axis=1)\n",
    "    for col in df.columns:\n",
    "        values = sorted(list(set(df[col].dropna())))\n",
    "        mapping = dict((v, i) for i, v in enumerate(values))\n",
    "        js['meta']['lookups'][col] = values\n",
    "        js['records'][col] = df[col].apply(lambda x: -1 if pd.isnull(x) else mapping[x]).tolist()\n",
    "\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_new(df, id_col=None):\n",
    "    def get_age_group(age):\n",
    "        if age == 'not given':\n",
    "            return age\n",
    "        elif age < 18:\n",
    "            return 'under 18'\n",
    "        elif age < 30:\n",
    "            return '18 to 29'\n",
    "        elif age < 40:\n",
    "            return '30 to 39'\n",
    "        elif age < 50:\n",
    "            return '40 to 49'\n",
    "        elif age < 60:\n",
    "            return '50 to 59'\n",
    "        else:\n",
    "            return '60 and up'\n",
    "\n",
    "    js = {'records': {}}\n",
    "    if id_col:\n",
    "        df = df.drop(id_col, axis=1)\n",
    "    for col in df.columns:\n",
    "        js['records'][col] = df[col].fillna('not given').tolist()\n",
    "        if col in ('age_at_time_of_death', 'civilian_age', 'officer_age'):\n",
    "            js['records']['age_group'] = df[col].apply(lambda x: get_age_group(x)).tolist()\n",
    "\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_slider_data_to_s3(slider_data):\n",
    "    print(\"Uploading all slider data to s3\")\n",
    "    s3 = boto3.resource('s3')\n",
    "    slider_file = s3.Object('tji-compressed-data', 'all_slider_data.json')\n",
    "    slider_file.put(Body=json.dumps(slider_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one(config, sample=False, s3_upload=False, accum_slider_data=None):\n",
    "    datasets = dw.load_dataset(config['DW_PROJECT_KEY'], force_update=True)\n",
    "    df = datasets.dataframes[config['DW_FILENAME']]\n",
    "\n",
    "    slim = df.copy()\n",
    "    slim['year'] = pd.to_datetime(slim[config['DATE_COL']]).dt.year\n",
    "    slim = slim[config['KEEP_COLS']]\n",
    "    slim.columns = [config.get('RENAMES', {}).get(c, c) for c in slim.columns]\n",
    "    prefix = \"\"\n",
    "    if sample:\n",
    "        slim = slim.sample(5)\n",
    "        prefix = \"SAMPLE_\"\n",
    "    \n",
    "    compressed = compress_original(slim, id_col=config['ID_COL'])\n",
    "    compressed_new = compress_new(slim, id_col=config['ID_COL'])\n",
    "    # Write\n",
    "    if not s3_upload:\n",
    "        filename = OUTFOLDER + prefix + config['OUTFILE_PREFIX'] + '_compressed.json'\n",
    "        print(\"Writing file to\", filename)\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(json.dumps(compressed))\n",
    "\n",
    "        filename = OUTFOLDER + prefix + config['OUTFILE_PREFIX'] + '_compressed_new.json'\n",
    "        print(\"Writing file to\", filename)\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(json.dumps(compressed_new))\n",
    "\n",
    "        filename = OUTFOLDER + prefix + config['OUTFILE_PREFIX'] + '_compressed_new.json'\n",
    "        print(\"Writing file to\", filename + '.gz')\n",
    "        with open(filename, 'rb') as f_in, gzip.open(filename + '.gz', 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        fullfile = OUTFOLDER + prefix + config['OUTFILE_PREFIX'] + '_full.csv'\n",
    "        print(\"Writing file to \" + fullfile)\n",
    "        df.to_csv(fullfile, index=False)\n",
    "    else:\n",
    "        s3 = boto3.resource('s3')\n",
    "        slider_data = {\n",
    "            'startingYear': compressed['meta']['lookups']['year'][0],\n",
    "            'totalRecords': compressed['meta']['num_records']\n",
    "        }\n",
    "        accum_slider_data[config['OUTFILE_PREFIX']] = slider_data\n",
    "        s3_filename = prefix + config['OUTFILE_PREFIX'] + '_compressed.json'\n",
    "        print(\"Uploading file \" + s3_filename + \" to s3\")\n",
    "        s3.Bucket(S3_BUCKET_NAME).upload_file(s3_filename, s3_filename)\n",
    "\n",
    "        s3_filename = prefix + config['OUTFILE_PREFIX'] + '_compressed_new.json'\n",
    "        print(\"Uploading file \" + s3_filename + \" to s3\")\n",
    "        s3.Bucket(S3_BUCKET_NAME).upload_file(s3_filename, s3_filename)\n",
    "                     \n",
    "        s3_filename = prefix + config['OUTFILE_PREFIX'] + '_compressed_new.json.gz'\n",
    "        print(\"Uploading file \" + s3_filename + \" to s3\")\n",
    "        s3.Bucket(S3_BUCKET_NAME).upload_file(s3_filename, s3_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdr\n",
      "Writing file to ./SAMPLE_cdr_compressed.json\n",
      "Writing file to ./SAMPLE_cdr_compressed_new.json\n",
      "Writing file to ./SAMPLE_cdr_compressed_new.json.gz\n",
      "Writing file to ./SAMPLE_cdr_full.csv\n",
      "Writing file to ./cdr_compressed.json\n",
      "Writing file to ./cdr_compressed_new.json\n",
      "Writing file to ./cdr_compressed_new.json.gz\n",
      "Writing file to ./cdr_full.csv\n",
      "ois-civilians\n",
      "Writing file to ./SAMPLE_ois_compressed.json\n",
      "Writing file to ./SAMPLE_ois_compressed_new.json\n",
      "Writing file to ./SAMPLE_ois_compressed_new.json.gz\n",
      "Writing file to ./SAMPLE_ois_full.csv\n",
      "Writing file to ./ois_compressed.json\n",
      "Writing file to ./ois_compressed_new.json\n",
      "Writing file to ./ois_compressed_new.json.gz\n",
      "Writing file to ./ois_full.csv\n",
      "ois-officers\n",
      "Writing file to ./SAMPLE_ois_officers_compressed.json\n",
      "Writing file to ./SAMPLE_ois_officers_compressed_new.json\n",
      "Writing file to ./SAMPLE_ois_officers_compressed_new.json.gz\n",
      "Writing file to ./SAMPLE_ois_officers_full.csv\n",
      "Writing file to ./ois_officers_compressed.json\n",
      "Writing file to ./ois_officers_compressed_new.json\n",
      "Writing file to ./ois_officers_compressed_new.json.gz\n",
      "Writing file to ./ois_officers_full.csv\n"
     ]
    }
   ],
   "source": [
    "for k, config in CONFIGS.items():\n",
    "    print(k)\n",
    "    create_one(config, sample=True)\n",
    "    create_one(config, sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For cronjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file cdr_compressed.json to s3\n",
      "Uploading file ois_compressed.json to s3\n",
      "Uploading file ois_officers_compressed.json to s3\n",
      "Uploading all slider data to s3\n"
     ]
    }
   ],
   "source": [
    "all_slider_data = {}\n",
    "for dataset, configs in CONFIG_MAPPING.items():\n",
    "    if os.environ.get('COMPRESS_%s_S3' % dataset.upper()) != 'TRUE':\n",
    "        print(\"Not writing to s3. To do so, set COMPRESS_%s_S3 to 'TRUE'\" % dataset.upper())\n",
    "    else:\n",
    "        for config in configs:\n",
    "            create_one(CONFIGS[config], s3_upload=True, accum_slider_data=all_slider_data)\n",
    "if all_slider_data:\n",
    "    write_slider_data_to_s3(all_slider_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
